{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Kode ini membangun model yang mengimplementasikan dasar Convolutional Neural Network (CNN) untuk klasifikasi gambar biner.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS TUF15\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413ms/step - accuracy: 0.5523 - loss: 0.7532"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS TUF15\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 475ms/step - accuracy: 0.5525 - loss: 0.7529 - val_accuracy: 0.6633 - val_loss: 0.6317\n",
      "Epoch 2/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 639us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.6250 - val_loss: 0.8024\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS TUF15\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 257ms/step - accuracy: 0.6435 - loss: 0.6351 - val_accuracy: 0.6290 - val_loss: 0.6449\n",
      "Epoch 4/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4375 - val_loss: 0.7407\n",
      "Epoch 5/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 262ms/step - accuracy: 0.6637 - loss: 0.6173 - val_accuracy: 0.7077 - val_loss: 0.5795\n",
      "Epoch 6/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.5625 - val_loss: 0.6583\n",
      "Epoch 7/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 242ms/step - accuracy: 0.7071 - loss: 0.5703 - val_accuracy: 0.6925 - val_loss: 0.6256\n",
      "Epoch 8/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.4375 - val_loss: 0.8788\n",
      "Epoch 9/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 267ms/step - accuracy: 0.7162 - loss: 0.5505 - val_accuracy: 0.7434 - val_loss: 0.5240\n",
      "Epoch 10/10\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.8125 - val_loss: 0.4722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x178985046d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mengimpor library yang diperlukan\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input\n",
    "\n",
    "# Inisialisasi CNN\n",
    "MesinKlasifikasi = Sequential()\n",
    "\n",
    "# Langkah 1 - Convolution\n",
    "MesinKlasifikasi.add(Input(shape=(128, 128, 3)))\n",
    "MesinKlasifikasi.add(Conv2D(filters = 32, kernel_size=(3, 3), activation = 'relu'))\n",
    "\n",
    "# Langkah 2 - Pooling\n",
    "MesinKlasifikasi.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Menambah convolutional layer\n",
    "MesinKlasifikasi.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "MesinKlasifikasi.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Langkah 3 - Flattening\n",
    "MesinKlasifikasi.add(Flatten())\n",
    "\n",
    "# Langkah 4 - Full connection\n",
    "MesinKlasifikasi.add(Dense(units = 128, activation = 'relu'))\n",
    "MesinKlasifikasi.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Menjalankan CNN\n",
    "MesinKlasifikasi.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Menjalankan CNN ke training dan test set\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (128, 128),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (128, 128),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "MesinKlasifikasi.fit(training_set,\n",
    "                         steps_per_epoch = 8000//32,\n",
    "                         epochs = 10,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 2000//32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model ini menggunakan pustaka Keras dari TensorFlow untuk membangun CNN dan memproses dataset gambar. Model terdiri dari beberapa langkah:\n",
    "1. Insialisasi CNN : Membuat model sekuensial di mana lapisan ditambahkan secara bertahap.\n",
    "1. Convolution : Mengekstrak fitur penting dari gambar menggunakan filter (kernel) 3x3.\n",
    "2. Max Pooling : Mengurangi ukuran data sambil mempertahankan fitur utama.\n",
    "3. Menambah Convolutional layer : Tambahan layer konvolusi dan pooling untuk memperdalam jaringan, meningkatkan kemampuan ekstraksi fitur.\n",
    "4. Flattening : Mengubah data matriks menjadi vektor 1 kolom untuk input ke neural network.\n",
    "5. Full Connection : Membentuk lapisan fully connected dengan neuron, termasuk lapisan output yang menggunakan fungsi aktivasi sigmoid untuk klasifikasi biner.\n",
    "\n",
    "Model ini dikompilasi menggunakan optimasi Adam untuk mempercepat proses konvergensi, fungsi loss binary_crossentropy untuk menghitung kesalahan, dan metrik accuracy untuk mengevaluasi performa. Dataset untuk pelatihan dan validasi diproses menggunakan ImageDataGenerator, yang melakukan augmentasi data seperti rescale, shear, zoom, dan flip horizontal pada data pelatihan untuk mencegah overfitting. Model dilatih menggunakan metode fit, dengan batch size 32, selama 10 epoch (bisa ditambah menjadi 50).\n",
    "\n",
    "*Dataset dapat dilihat pada folder dataset*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
